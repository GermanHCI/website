@inproceedings{CarrosExploring,
title = {Exploring Human-Robot Interaction with the Elderly: Results from a Ten-Week Case Study in a Care Home},
author = {Felix Carros (Uni Siegen) and Johanna Meurer (Uni Siegen) and Diana Löffler (Uni Siegen) and David Unbehaun (Uni Siegen) and Sarah Matthies (Uni Siegen) and Inga Koch (Uni Siegen) and Rainer Wieching (Uni Siegen) and Dave Randall (Uni Siegen) and Marc Hassenzahl (Uni Siegen) and Volker Wulf (Uni Siegen)},
doi = {10.1145/3313831.3376402},
year  = {2020},
date = {2020-05-01},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@workshop{KumarSelf,
title = {SelfSustainableCHI: Self-Powered Sustainable Interfaces and Interactions},
author = {Yogesh Kumar Meena (Swansea University) and Xing-Dong Yang (Dartmouth College) and Markus Löchtefeld (Aalborg University) and Matt Carnie (Swansea University) and Niels Henze (University of Regensburg) and Steve Hodges (Microsoft Research) and Matt Jones (Swansea University) and Nivedita Arora (Georgia Institute of Technolgy) and Gregory D. Abowd (Georgia Institute of Technology)},
doi = {10.1145/3334480.3375167},
year  = {2020},
date = {2020-05-01},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The continued proliferation of computing devices comes with an ever-increasing energy requirement, both during production and use. As awareness of the global climate emergency increases, self-powered and sustainable (SelfSustainable) interactive devices are likely to play a valuable role. In this workshop we bring together researchers and practitioners from design, computer science, materials science, engineering and manufacturing industries working on this new area of endeavour. The workshop will provide a platform for participants to review and discuss challenges and opportunities associated with self-powered and sustainable interfaces and interactions, develop a design space and identify opportunities for future research.},
keywords = {Workshop},
pubstate = {published},
tppubtype = {workshop}
}
@inproceedings{KurzeGuess,
title = {Guess the Data: Data Work to Understand How People Make Sense of and Use Simple Sensor Data from Homes},
author = {Albrecht Kurze (Chemnitz University of Technology) and Andreas Bischof (Chemnitz University of Technology) and Sören Totzauer (Chemnitz University of Technology) and Michael Storz (Chemnitz University of Technology) and Maximilian Eibl (Chemnitz University of Technology) and Margot Brereton (Queensland University of Technology) and Arne Berger (Anhalt University of Applied Sciences)},
url = {https://www.twitter.com/arneberger, Twitter},
doi = {10.1145/3313831.3376273},
year  = {2020},
date = {2020-05-01},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Simple smart home sensors, e.g. for temperature or light, increasingly collect seemingly inconspicuous data. Prior work has shown that human sensemaking of such sensor data can reveal domestic activities. Such sensemaking presents an opportunity to empower people to understand the implications of simple smart home sensors. To investigate, we developed and field-tested the Guess the Data method, which enabled people to use and make sense of live data from their homes and to collectively interpret and reflect on anonymized data from the homes in our study. Our findings show how participants reconstruct behavior, both individually and collectively, expose the sensitive personal data of others, and use sensor data as evidence and for lateral surveillance within the household. We discuss the potential of our method as a participatory HCI method for investigating design of the IoT and implications created by doing data work on home sensors.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KlamkaWatchStrap,
title = {Watch+Strap: Extending Smartwatches with Interactive StrapDisplays},
author = {Konstantin Klamka (Technische Universität Dresden) and Tom Horak (Technische Universität Dresden) and Raimund Dachselt (Technische Universität Dresden) },
url = {https://youtu.be/Op8-gh5GSxI, Video
https://www.twitter.com/imldresden, Twitter},
doi = {10.1145/3313831.3376199},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
institution = {TU Dresden},
abstract = {While smartwatches are widely adopted these days, their input and output space remains fairly limited by their screen size. We present StrapDisplays—interactive watchbands with embedded display and touch technologies—that enhance commodity watches and extend their input and output capabilities. After introducing the physical design space of these StrapDisplays, we explore how to combine a smartwatch and straps in a synergistic Watch+Strap system. Specifically, we propose multiple interface concepts that consider promising content distributions, interaction techniques, usage types, and display roles. For example, the straps can enrich watch apps, display visualizations, provide glanceable feedback, or help avoiding occlusion issues. Further, we provide a modular research platform incorporating three StrapDisplay prototypes and a flexible web-based software architecture, demonstrating the feasibility of our approach. Early brainstorming sessions with 15 participants informed our design process, while later interviews with six experts supported our concepts and provided valuable feedback for future developments.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KlamkaRapid,
title = {Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes},
author = {Konstantin Klamka (Technische Universität Dresden) and Raimund Dachselt (Technische Universität Dresden) and Jürgen Steimle (Saarland University)},
url = {https://youtu.be/FyPcMLBXIm0, Video
https://www.twitter.com/imldresden, Twitter},
doi = {10.1145/3313831.3376220},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
institution = {TU Dresden},
abstract = {Rapid prototyping of interactive textiles is still challenging, since manual skills, several processing steps, and expert knowledge are involved. We present Rapid Iron-On User Interfaces, a novel fabrication approach for empowering designers and makers to enhance fabrics with interactive functionalities. It builds on heat-activated adhesive materials consisting of smart textiles and printed electronics, which can be flexibly ironed onto the fabric to create custom interface functionality. To support rapid fabrication in a sketching-like fashion, we developed a handheld dispenser tool for directly applying continuous functional tapes of desired length as well as discrete patches. We introduce versatile compositions techniques that allow for creating complex circuits, utilizing commodity textile accessories and sketching custom-shaped I/O modules. We further contribute a comprehensive library of components for input, output, wiring and computing. Three example applications, results from technical experiments and expert reviews demonstrate the functionality, versatility and potential of this approach.},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{DuerrNurseCare,
title = {NurseCare: Design and ‘In-The-Wild’ Evaluation of a Mobile System to Promote the Ergonomic Transfer of Patients},
author = {Maximilian Dürr (University of Konstanz) and Carla Gröschel (University of Konstanz) and Ulrike Pfeil (University of Konstanz) and Harald Reiterer (University of Konstanz)},
url = {https://youtu.be/BJaKsSOjW4k, Video
https://www.twitter.com/HCIGroupKN, Twitter},
doi = {10.1145/3313831.3376851},
year  = {2020},
date = {2020-04-26},
urldate = {2020-04-07},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
institution = {University of Konstanz},
abstract = {Nurses are frequently required to transfer patients as part of their daily duties. However, the manual transfer of patients is a major risk factor for injuries to the back. Although the Kinaesthetics Care Conception can help to address this issue, existing support for the integration of the concept into nursing-care practice is low. We present NurseCare, a mobile system that aims to promote the practical application of ergonomic patient transfers based on the Kinaesthetics Care Conception. NurseCare consists of a wearable and a smartphone app. Key features of NurseCare include mobile accessible instructions for ergonomic patient transfers, in-situ feedback for the risky bending of the back, and long-term feedback. We evaluated NurseCare in a nine participant ‘in-the-wild’ evaluation. Results indicate that NurseCare can facilitate ergonomic work while providing a high user experience adequate to the nurses’ work domain, and reveal how NurseCare can be incorporated in given practices.},
type = {Full Paper},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MuellerIntegration,
title = {Next Steps in Human-Computer Integration},
author = {Florian 'Floyd' Mueller (Monash University) and Pedro Lopes (University of Chicago) and Paul Strohmeier (University of Copenhagen / Saarland University) and Wendy Ju (Cornell Tech) and Caitlyn Seim (Stanford University) and Martin Weigel (Honda Research Institute Europe) and Suranga Nanayakkara (University of Auckland) and Marianna Obrist (University of Essex) and Zhuying Li (Monash University) and Joseph Delfa (Monash University) and Jun Nishida (University of Chicago) and Elizabeth M. Gerber (Northwestern University) and Dag Svanaes (NTNU / IT University of Copenhagen) and Jonathan Grudin (Microsoft) and Stefan Greuter (Deakin University) and Kai Kunze (Keio University) and Thomas Erickson (Independent researcher) and Steven Greenspan (CA Technologies) and Masahiko Inami (University of Tokyo) and Joe Marshall (University of Nottingham) and Harald Reiterer (University of Konstanz) and Katrin Wolf (Beuth University of Applied Sciences Berlin) and Jochen Meyer (OFFIS) and Thecla Schiphorst (Simon Fraser University) and Dakuo Wang (IBM Research) and Pattie Maes (MIT Media Lab)},
doi = {10.1145/3313831.3376242},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Human-Computer Integration (HInt) is an emerging paradigm in which computational and human systems are closely interwoven. Integrating computers with the human body is not new. However, we believe that with rapid technological advancements, increasing real-world deployments, and growing ethical and societal implications, it is critical to identify an agenda for future research. We present a set of challenges for HInt research, formulated over the course of a five-day workshop consisting of 29 experts who have designed, deployed, and studied HInt systems. This agenda aims to guide researchers in a structured way towards a more coordinated and conscientious future of human-computer integration.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{ZagermannStudying,
title = {'It’s in my other hand!' - Studying the Interplay of Interaction Techniques and Multi-Tablet Activities},
author = {Johannes Zagermann (University of Konstanz) and Ulrike Pfeil (University of Konstanz) and Philipp von Bauer (University of Konstanz) and Daniel Fink (University of Konstanz) and Harald Reiterer (University of Konstanz)},
url = {https://youtu.be/_LZsSPP1FM4, Video
https://www.twitter.com/HCIGroupKN, Twitter},
doi = {10.1145/3313831.3376540},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Cross-device interaction with tablets is a popular topic in HCI research. Recent work has shown the benefits of including multiple devices into users’ workflows while various interaction techniques allow transferring content across devices. However, users are only reluctantly using multiple devices in combination. At the same time, research on cross-device interaction struggles to find a frame of reference to compare techniques or systems. In this paper, we try to address these challenges by studying the interplay of interaction techniques, device utilization, and task-specific activities in a user study with 24 participants from different but complementary angles of evaluation using an abstract task, a sensemaking task, and three interaction techniques. We found that different interaction techniques have a lower influence than expected, that work behaviors and device utilization depend on the task at hand, and that participants value specific aspects of cross-device interaction.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{PfauBot,
title = {Bot or not? User Perceptions of Player Substitution with Deep Player Behavior Models},
author = {Johannes Pfau (University of Bremen) and Jan David Smeddinck (Newcastle University) and Ioannis Bikas (University of Bremen) and Rainer Malaka (University of Bremen)},
url = {https://www.twitter.com/dmlabbremen, Twitter},
doi = {10.1145/3313831.3376223},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Many online games suffer when players drop off due to lost connections or quitting prematurely, which leads to match terminations or game-play imbalances. While rule-based outcome evaluations or substitutions with bots are frequently used to mitigate such disruptions, these techniques are often perceived as unsatisfactory. Deep learning methods have successfully been used in deep player behavior modelling (DPBM) to produce non-player characters or bots which show more complex behavior patterns than those modelled using traditional AI techniques. Motivated by these findings, we present an investigation of the player-perceived awareness, believability and representativeness, when substituting disconnected players with DPBM agents in an online-multiplayer action game. Both quantitative and qualitative outcomes indicate that DPBM agents perform similarly to human players and that players were unable to detect substitutions. In contrast, players were able to detect substitution with agents driven by more traditional heuristics.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{PfauEnemy,
title = {Enemy Within: Long-term Motivation Effects of Deep Player Behavior Models for Dynamic Difficulty Adjustment},
author = {Johannes Pfau (University of Bremen) and Jan David Smeddinck (Newcastle University) and Rainer Malaka (University of Bremen)},
url = {https://www.youtube.com/watch?v=QOdFmvQnPJQ, Video
https://www.twitter.com/dmlabbremen, Twitter},
doi = {10.1145/3313831.3376423},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Balancing games and producing content that remains interesting and challenging is a major cost factor in the design and maintenance of games. Dynamic difficulty adjustment (DDA) can successfully tune challenge levels to player abilities, but when implemented with classic heuristic parameter tuning (HPT) often turns out to be very noticeable, e.g. as “rubber-banding”. Deep learning techniques can be employed for deep player behavior modeling (DPBM), enabling more complex adaptivity, but effects over frequent and longer-lasting game engagements, as well as comparisons to HPT have not been empirically investigated. We present a situated study of the effects of DDA via DPBM as compared to HPT on intrinsic motivation, perceived challenge and player motivation in a real-world MMORPG. The results indicate that DPBM can lead to significant improvements in intrinsic motivation and players prefer game experience episodes featuring DPBM over experience episodes with classic difficulty management.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{PutzeBreaking,
title = {Breaking The Experience: Effects of Questionnaires in VR User Studies},
author = {Susanne Putze (University of Bremen) and Dmitry Alexandrovsky (University of Bremen) and Felix Putze (University of Bremen) and Sebastian Höffner (University of Bremen) and Jan David Smeddinck (Newcastle University) and Rainer Malaka (University of Bremen)},
url = {https://www.youtube.com/watch?v=iHdW3nphCZQ, Video
https://www.twitter.com/dmlabbremen, Twitter},
doi = {10.1145/3313831.3376144},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Questionnaires are among the most common research tools in virtual reality (VR) evaluations and user studies. However, transitioning from virtual worlds to the physical world to respond to VR experience questionnaires can potentially lead to systematic biases. Administering questionnaires in VR (inVRQs) is becoming more common in contemporary research. This is based on the intuitive notion that inVRQs may ease participation, reduce the Break in Presence (BIP) and avoid biases. In this paper, we perform a systematic investigation into the effects of interrupting the VR experience through questionnaires using physiological data as a continuous and objective measure of presence. In a user study (n=50), we evaluated question-asking procedures using a VR shooter with two different levels of immersion. The users rated their player experience with a questionnaire either inside or outside of VR. Our results indicate a reduced BIP for the employed INVRQ without affecting the self-reported player experience.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{AlexandrovskyExamining,
title = {Examining Design Choices of Questionnaires in VR User Studies},
author = {Dmitry Alexandrovsky (University of Bremen) and Susanne Putze (University of Bremen) and Michael Bonfert (University of Bremen) and Sebastian Höffner (University of Bremen) and Pitt Michelmann (University of Bremen) and Dirk Wenig (University of Bremen) and Rainer Malaka (University of Bremen) and Jan David Smeddinck (Newcastle University)},
url = {https://www.youtube.com/watch?v=T32Sop_LFu0&feature=youtu.be, Video
https://www.twitter.com/dmlabbremen, Twitter},
doi = {10.1145/3313831.3376260},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Questionnaires are among the most common research tools in virtual reality (VR) user studies. Transitioning from virtuality to reality for giving self-reports on VR experiences can lead to systematic biases. VR allows to embed questionnaires into the virtual environment which may ease participation and avoid biases. To provide a cohesive picture of methods and design choices for questionnaires in VR (inVRQ), we discuss 15 inVRQ studies from the literature and present a survey with 67 VR experts from academia and industry. Based on the outcomes, we conducted two user studies in which we tested different presentation and interaction methods of inVRQs and evaluated the usability and practicality of our design. We observed comparable completion times between inVRQs and questionnaires outside VR (outVRQs) with higher enjoyment but lower usability for INVRQS. These findings advocate the application of INVRQS and provide an overview of methods and considerations that lay the groundwork for inVRQ design.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{SchroederRobustness,
title = {Robustness of Eye Movement Biometrics Against Varying Stimuli and Varying Trajectory Length},
author = {Christoph Schröder (University of Bremen) and Sahar Mahdie Klim Al Zaidawi (University of Bremen) and Martin H.U. Prinzler (University of Bremen) and Sebastian Maneth (University of Bremen) and Gabriel Zachmann (University of Bremen)},
doi = {10.1145/3313831.3376534},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Recent results suggest that biometric identification based on human’s eye movement characteristics can be used for authentication. In this paper, we present three new methods and benchmark them against the state-of-the-art. The best of our new methods improves the state-of-the-art performance by 5.9 percentage points. Furthermore, we investigate some of the factors that affect the robustness of the recognition rate of different classifiers on gaze trajectories, such as the type of stimulus and the tracking trajectory length. We find that the state-of-the-art method only works well when using the same stimulus for testing that was used for training. By contrast, our novel method more than doubles the identification accuracy for these transfer cases. Furthermore, we find that with only 90 seconds of eye tracking data, 86.7 % accuracy can be achieved.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{PutzePlatform,
title = {Platform for Studying Self-Repairing Auto-Corrections in Mobile Text Entry based on Brain Activity, Gaze, and Context},
author = {Felix Putze (University of Bremen) and Tilman Ihrig (University of Bremen) and Tanja Schultz (University of Bremen) and Wolfganz Stuerzlinger (Simon Fraser University)},
doi = {10.1145/3313831.3376815},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Auto-correction is a standard feature of mobile text entry. While the performance of state-of-the-art auto-correct methods is usually relatively high, any errors that occur are cumbersome to repair, interrupt the flow of text entry, and challenge the user's agency over the process. In this paper, we describe a system that aims to automatically identify and repair auto-correction errors. This system comprises a multi-modal classifier for detecting auto-correction errors from brain activity, eye gaze, and context information, as well as a strategy to repair such errors by replacing the erroneous correction or suggesting alternatives. We integrated both parts in a generic Android component and thus present a research platform for studying self-repairing end-to-end systems. To demonstrate its feasibility, we performed a user study to evaluate the classification performance and usability of our approach.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{FrommelRecognizing,
title = {Recognizing Affiliation: Using Behavioural Traces to Predict the Quality of Social Interactions in Online Games},
author = {Julian Frommel (Ulm University / University of Saskatchewan) and Valentin Sagl (University of Saskatchewan) and Ansgar E. Depping (University of Saskatchewan) and Colby Johanson (University of Saskatchewan) and Matthew K. Miller (University of Saskatchewan) and Regan L. Mandryk (University of Saskatchewan)},
url = {https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376446},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Online social interactions in multiplayer games can be supportive and positive or toxic and harmful; however, few methods caneasily assess interpersonal interaction quality in games. We use behavioural traces to predict affiliation between dyadic strangers, facilitated through their social interactions in an online gaming setting. We collected audio, video, in-game, and self-report data from 23 dyads, extracted 75 features, trained Random Forest and Support VectorMachine models, and evaluated their performance predicting binary (high/low) as well as continuous affiliation toward a partner. The models can predict both binary and continuous affiliation with up to 79.1% accuracy (F1) and 20.1% explained variance (R2) on unseen data, with features based on verbal communication demonstrating the highest potential. Our findings can inform the design of multiplayer games and game communities, and guide the development of systems for matchmaking and mitigating toxic behaviour in online games.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{WolfUnderstanding,
title = {Understanding the Heisenberg Effect of Spatial Interaction: A Selection Induced Error for Spatially Tracked Input Devices},
author = {Dennis Wolf (Ulm University) and Jan Gugenheimer (Ulm University) and Marco Combosch (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376876},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Virtual and augmented reality head-mounted displays (HMDs) are currently heavily relying on spatially tracked input devices (STID) for interaction. These STIDs are all prone to the phenomenon that a discrete input (e.g., button press) will disturb the position of the tracker, resulting in a different selection point during ray-cast interaction (Heisenberg Effect of Spatial Interaction). Besides the knowledge of its existence, there is currently a lack of a deeper understanding of its severity, structure and impact on throughput and angular error during a selection task. In this work, we present a formal evaluation of the Heisenberg effect and the impact of body posture, arm position and STID degrees of freedom on its severity. In a Fitt’s law inspired user study (N=16), we found that the Heisenberg effect is responsible for 30.45% of the overall errors occurring during a pointing task, but can be reduced by 25.4% using a correction function.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{WolfJump,
title = {JumpVR: Jump-Based Locomotion Augmentation for Virtual Reality},
author = {Dennis Wolf (Ulm University) and Katja Rogers (Ulm University) and Christoph Kunder (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://youtu.be/JNWfs3-V1zQ, Video
https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376243},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {One of the great benefits of virtual reality (VR) is the implementation of features that go beyond realism. Common “unrealistic” locomotion techniques (like teleportation) can avoid spatial limitation of tracking but minimize potential benefits of more realistic techniques (e.g., walking). As an alternative that combines realistic physical movement with hyper-realistic virtual outcome, we present JumpVR, a jump-based locomotion augmentation technique that virtually scales users’ physical jumps. In a user study (N=28), we show that jumping in VR (regardless of scaling) can significantly increase presence, motivation and immersion compared to teleportation, while largely not increasing simulator sickness. Further, participants reported higher immersion and motivation for most scaled jumping variants than forward-jumping. Our work shows the feasibility and benefits of jumping in VR and explores suitable parameters for its hyper-realistic scaling. We discuss design implications for VR experiences and research.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{StemasovMix,
title = {Mix&Match: Towards Omitting Modelling through In-Situ Alteration and Remixing of Model Repository Artifacts in Mixed Reality},
author = {Evgeny Stemasov (Ulm University) and Tobias Wagner (Ulm University) and Jan Gugenheimer (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://youtu.be/Dyb0QRtNtag, Video
https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376839},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The accessibility of tools to model artifacts is one of the core driving factors for the adoption of Personal Fabrication. Subsequently, model repositories like Thingiverse became important tools in (novice) makers' processes. They allow them to shorten or even omit the design process, offloading a majority of the effort to other parties. However, steps like measurement of surrounding constraints (e.g., clearance) which exist only inside the users' environment, can not be similarly outsourced. We propose Mix&Match a mixed-reality-based system which allows users to browse model repositories, preview the models in-situ, and adapt them to their environment in a simple and immediate fashion. Mix&Match aims to provide users with CSG operations which can be based on both virtual and real geometry. We present interaction patterns and scenarios for Mix&Match, arguing for the combination of mixed reality and model repositories. This enables almost modelling-free personal fabrication for both novices and expert makers.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{ColleyTowards,
title = {Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments},
author = {Mark Colley (Ulm University) and Marcel Walch (Ulm University) and Jan Gugenheimer (Ulm University) and Ali Askari (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://youtu.be/1L7zTJ86PE8, Video
https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376472},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {People with vision impairments (VIP) are among the most vulnerable road users in traffic. Autonomous vehicles are believed to reduce accidents but still demand some form of external communication signaling relevant information to pedestrians. Recent research on the design of vehicle-pedestrian communication (VPC) focuses strongly on concepts for a non-disabled population. Our work presents an inclusive user-centered design for VPC, beneficial for both vision impaired and seeing pedestrians. We conducted a workshop with VIP (N=6), discussing current issues in road traffic and comparing communication concepts proposed by literature. A thematic analysis unveiled two important themes: number of communicating vehicles and content (affecting duration). Subsequently, we investigated these in a second user study in virtual reality (N=33, 8 VIP) comparing the VPC between groups of abilities. We found that trust and understanding is enhanced and cognitive load reduced when all relevant vehicles communicate; high content messages also reduce cognitive load.},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{DreyVRSketchIn,
title = {VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality},
author = {Tobias Drey (Ulm University) and Jan Gugenheimer (Ulm University) and Julian Karlbauer (Ulm University) and Maximilian Milo (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://youtu.be/99hIlAbfan4, Video
https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376628},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{RietzlerTelewalk,
title = {Telewalk: Towards Free and Endless Walking in Room-Scale Virtual Reality},
author = {Michael Rietzler (Ulm University) and Martin Deubzer (Ulm University) and Thomas Dreja (Ulm University) and Enrico Rukzio (Ulm University)},
url = {https://www.twitter.com/mi_uulm, Twitter},
doi = {10.1145/3313831.3376821},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Natural navigation in VR is challenging due to spatial limitations. While Teleportation enables navigation within very small physical spaces and without causing motion sickness symptoms, it may reduce the feeling of presence and spacial awareness. Redirected walking (RDW), in contrast, allows users to naturally walk while staying inside a finite, but still very large, physical space. We present Telewalk, a novel locomotion approach that combines curvature and translation gains known from RDW research in a perceivable way. This combination enables Telewalk to be applied even within a physical space of 3m x 3m. Utilizing the head rotation as input device enables directional changes without any physical turns to keep the user always on an optimal circular path inside the real world while freely walking inside the virtual one. In a user study we found that even though motion sickness susceptible participants reported respective symptoms, Telewalk did result in stronger feelings of presence and immersion and was seen as more natural then Teleportation.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{OsmersGetting,
title = {Getting out of Out of Sight: Evaluation of AR Mechanisms for Awareness and Orientation Support in Occluded Multi-Room Settings},
author = {Niklas Osmers (TU Clausthal) and Michael Prilla (TU Clausthal)},
url = {https://www.twitter.com/HCISGroup, Twitter},
doi = {10.1145/3313831.3376742},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Augmented Reality can provide orientation and awareness in situations in which objects or people are occluded by physical structures. This is relevant for many situations in the workplace, where objects are scattered across rooms and people are out of sight. While several AR mechanisms have been proposed to provide awareness and orientation in these situations, little is known about their effect on people's performance when searching objects and coordinating with each other. In this paper, we compare three AR based mechanisms (map, x-ray, compass) according to their utility, usability, social presence, task load and users’ preferences. 48 participants had to work together in groups of four to find people and objects located around different rooms. Results show that map and x-ray performed best but provided least social presence among participants. We discuss these and other observations as well as potential impacts on designing AR awareness and orientation support.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{BuettnerAugmented,
title = {Augmented Reality Training for Industrial Assembly Work – Are Projection-based AR Assistive Systems an Appropriate Tool for Assembly Training?},
author = {Sebastian Büttner (TU Clausthal / TH OWL) and Michael Prilla (TU Clausthal) and Carsten Röcker (TH OWL)},
url = {https://www.twitter.com/HCISGroup},
doi = {10.1145/3313831.3376720},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {"Augmented Reality (AR) systems are on their way to industrial application, e.g. projection-based AR is used to enhance assembly work. Previous studies showed advantages of the systems in permanent-use scenarios, such as faster assembly times. 
In this paper, we investigate whether such systems are suitable for training purposes. Within an experiment, we observed the training with a projection-based AR system over multiple sessions and compared it with a personal training and a paper manual training. Our study shows that projection-based AR systems offer only small benefits in the training scenario. While a systematic mislearning of content is prevented through immediate feedback, our results show that the AR training does not reach the personal training in terms of speed and recall precision after 24 hours. Furthermore, we show that once an assembly task is properly trained, there are no differences in the long-term recall precision, regardless of the training method."},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{SubramanianTractus,
title = {TRACTUS: Understanding and Supporting Source Code Experimentation in Hypothesis-Driven Data Science},
author = {Krishna Subramanian (RWTH) and Johannes Maas (RWTH) and Jan Borchers (RWTH)},
url = {https://youtu.be/iP0aW731MUQ, Video},
doi = {10.1145/3313831.3376764},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Data scientists experiment heavily with their code, compromising code quality to obtain insights faster. We observed ten data scientists perform hypothesis-driven data science tasks, and analyzed their coding, commenting, and analysis practice. We found that they have difficulty keeping track of their code experiments. When revisiting exploratory code to write production code later, they struggle to retrace their steps and capture the decisions made and insights obtained, and have to rerun code frequently. To address these issues, we designed TRACTUS, a system extending the popular RStudio IDE, that detects, tracks, and visualizes code experiments in hypothesis-driven data science tasks. TRACTUS helps recall decisions and insights by grouping code experiments into hypotheses, and structuring information like code execution output and documentation. Our user studies show how TRACTUS improves data scientists' workflows, and suggest additional opportunities for improvement. TRACTUS is available as an open source RStudio IDE addin at http://hci.rwth-aachen.de/tractus.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{WackerHeatmaps,
title = {Heatmaps, Shadows, Bubbles, Rays: Comparing Mid-Air Pen Position Visualizations in Handheld AR},
author = {Philipp Wacker (RWTH) and Adrian Wagner (RWTH) and Simon Voelker (RWTH) and Jan Borchers (RWTH)},
url = {https://youtu.be/sFPP2xeAEP8, Video},
doi = {10.1145/3313831.3376848},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {In Handheld Augmented Reality, users look at AR scenes through the smartphone held in their hand. In this setting, having a mid-air pointing device like a pen in the other hand greatly expands the interaction possibilities. For example, it lets users create 3D sketches and models while on the go. However, perceptual issues in Handheld AR make it difficult to judge the distance of a virtual object, making it hard to align a pen to it. To address this, we designed and compared different visualizations of the pen's position in its virtual environment, measuring pointing precision, task time, activation patterns, and subjective ratings of helpfulness, confidence, and comprehensibility of each visualization. While all visualizations resulted in only minor differences in precision and task time, subjective ratings of perceived helpfulness and confidence favor a `heatmap' technique that colors the objects in the scene based on their distance to the pen.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{VoelkerGaze,
title = {GazeConduits: Calibration-Free Cross-Device Collaboration through Gaze and Touch},
author = {Simon Voelker (RWTH) and Sebastian Hueber (RWTH) and Christian Holz (ETH Zurich) and Christian Remy (Aarhus University) and Nicolai Marquardt (University College London)},
url = {https://youtu.be/Q59SQi0JUkg, Video},
doi = {10.1145/3313831.3376578},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {We present GazeConduits, a calibration-free ad-hoc mobile device setup that enables users to collaboratively interact with tablets, other users, and content in a cross-device setting using gaze and touch input. GazeConduits leverages recently presented phone capabilities to detect facial features and estimate users’ gaze directions. To join a collaborative setting, users place one or more tablets onto a shared table and position their phone in the center, which then tracks present users as well as their gaze direction to predict the tablets they look at. Using GazeConduits, we demonstrate a series of techniques for collaborative interaction across mobile devices for content selection and manipulation. Our evaluation with 20 simultaneous tablets on a table showed that GazeConduits can reliably identify at which tablet or at which collaborator a user is looking, enabling a rich set of interaction techniques.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{VoelkerHeadReach,
title = {HeadReach: Using Head Tracking to Increase Reachability on Mobile Touch Devices},
author = {Simon Voelker (RWTH) and Sebastian Hueber (RWTH) and Christian Corsten (RWTH) and Christian Remy (Aarhus University)},
url = {https://youtu.be/IyVp5VFde2w, Video},
doi = {10.1145/3313831.3376868},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {People often operate their smartphones with only one hand, using just their thumb for touch input. With today’s larger smartphones, this leads to a reachability issue: Users can no longer comfortably touch everywhere on the screen without changing their grip. We investigate using the head tracking in modern smartphones to address this reachability issue. We developed three interaction techniques, pure head (PH), head+ touch (HT), and head area + touch (HA), to select targets beyond the reach of one’s thumb. In two user studies, we found that selecting targets using HT and HA had higher success rates than the default direct touch (DT) while standing (by about 9%) and walking (by about 12%), while being moderately slower. HT and HA were also faster than one of the best techniques, BezelCursor (BC) (by about 20% while standing and 6% while walking), while having the same success rate.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{Marky3D,
title = {3D-Auth: Two-Factor Authentication with Personalized 3D-Printed Items},
author = {Karola Marky (TU Darmstadt) and Martin Schmitz (TU Darmstadt) and Verena Zimmer (TU Darmstadt) and Martin Herbers (TU Darmstadt) and Kai Kunze (Keio Media Design) and Max Mühlhäuser (TU Darmstadt)},
url = {https://youtu.be/_dHihnJTRek, Video
https://twitter.com/search?q=%23teamdarmstadt&src=typed_query&f=live, Twitter},
doi = {10.1145/3313831.3376189},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Two-factor authentication is a widely recommended security mechanism and already offered for different services. However, known methods and physical realizations exhibit considerable usability and customization issues. In this paper, we propose 3D-Auth, a new concept of two-factor authentication. 3D-Auth is based on customizable 3D-printed items that combine two authentication factors in one object. The object bottom contains a uniform grid of conductive dots that are connected to a unique embedded structure inside the item. Based on the interaction with the item, different dots turn into touch-points and form an authentication pattern. This pattern can be recognized by a capacitive touchscreen. Based on an expert design study, we present an interaction space with six categories of possible authentication interactions. In a user study, we demonstrate the feasibility of 3D-Auth items and show that the items are easy to use and the interactions are easy to remember.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{WillichPodoportation,
title = {Podoportation: Foot-Based Locomotion in Virtual Reality},
author = {Julius von Willich (TU Darmstadt) and Martin Schmitz (TU Darmstadt) and Florian Müller (TU Darmstadt) and Daniel Schmitt (TU Darmstadt) and Max Mühlhäuser (TU Darmstadt)},
url = {https://youtu.be/HGP5MN_e-k0, Video
https://twitter.com/search?q=%23teamdarmstadt&src=typed_query&f=live, Twitter},
doi = {10.1145/3313831.3376626},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based and hands-free locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point & teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for hands-free and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MarkyImproving,
title = {Improving the Usability and UX of the Swiss Internet Voting Interface},
author = {Karola Marky (TU Darmstadt) and Verena Zimmermann (TU Darmstadt) and Markus Funk (Cerence GmbH) and Jörg Daubert (TU Darmstadt) and Kira Bleck (TU Darmstadt) and Max Mühlhäuser (TU Darmstadt)},
url = {https://twitter.com/search?q=%23teamdarmstadt&src=typed_query&f=live, Twitter},
doi = {10.1145/3313831.3376769},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Up to 20% of residential votes and up to 70% of absentee votes in Switzerland are cast online. The Swiss scheme aims to provide individual verifiability by different verification codes. The voters have to carry out verification on their own, making the usability and UX of the interface of great importance. To improve the usability, we first performed an evaluation with 12 human-computer interaction experts to uncover usability weaknesses of the Swiss Internet voting interface. Based on the experts' findings, related work, and an exploratory user study with 36 participants, we propose a redesign that we evaluated in a user study with 49 participants. Our study confirmed that the redesign indeed improves the detection of incorrect votes by 33% and increases the trust and understanding of the voters. Our studies furthermore contribute important recommendations for designing verifiable e-voting systems in general.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{GuentherTherminator,
title = {Therminator: Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality},
author = {Sebastian Günther (TU Darmstadt) and Florian Müller (TU Darmstadt) and Dominik Schön (TU Darmstadt) and Omar Elmoghazy (GUC) and Martin Schmitz (TU Darmstadt) and Max Mühlhäuser (TU Darmstadt)},
url = {https://www.youtube.com/watch?v=w9FnG1eoWD8&feature=youtu.be, Video
https://twitter.com/search?q=%23teamdarmstadt&src=typed_query&f=live, Twitter},
doi = {10.1145/3313831.3376195},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MuellerWalk,
title = {Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays},
author = {Florian Müller (TU Darmstadt) and Martin Schmitz (TU Darmstadt) and Daniel Schmitt (TU Darmstadt) and Sebastian Günther (TU Darmstadt) and Markus Funk (TU Darmstadt) and Max Mühlhäuser (TU Darmstadt)},
url = {https://youtu.be/6-XrF6J9cTc, Video
https://twitter.com/search?q=%23teamdarmstadt&src=typed_query&f=live, Twitter},
doi = {10.1145/3313831.3376852},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user's walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MostajeranAugmented,
title = {Augmented Reality for Older Adults: Exploring Acceptability of Virtual Coaches for Home-based Balance Training in an Aging Population},
author = {Fariba Mostajeran (Uni Hamburg) and Frank Steinicke (Uni Hamburg) and Oscar Ariza (Uni Hamburg) and Dimitrios Gatsios (University of Ioannina) and Dimitrios Fotiadis (University of Ioannina)},
doi = {10.1145/3313831.3376565},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Balance training has been shown to be effective in reducing risks of falling, which is a major concern for older adults. Usually, exercise programs are individually prescribed and monitored by physiotherapeutic or medical experts. Unfortunately, supervision and motivation of older adults during home-based exercises cannot be provided on a large scale, in particular, considering an ageing population. Augmented reality (AR) in combination with virtual coaches could provide a reasonable solution to this challenge. We present a first investigation of the acceptance of an AR coaching system for balance training, which can be performed at home. In a human-centered design approach we developed several mock-ups and prototypes, and evaluated them with 76 older adults. The results suggest that older adults find the system encouraging and stimulating. The virtual coach is perceived as an alive, calm, intelligent, and friendly human. However, usability of the entire AR system showed a significant negative correlation with participants' age.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{FreiwaldWalking,
title = {Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated Virtual Reality Experiences},
author = {Jann Philipp Freiwald (Uni Hamburg) and Oscar Ariza (Uni Hamburg) and Omar Janeh (Uni Hamburg) and Frank Steinicke (Uni Hamburg)},
doi = {10.1145/3313831.3376574},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {We introduce VR Strider, a novel locomotion user interface (LUI) for seated virtual reality (VR) experiences, which maps cycling biomechanics of the user's legs to virtual walking movements. The core idea is to translate the motion of pedaling on a mini exercise bike to a corresponding walking animation of a virtual avatar while providing audio-based tactile feedback on virtual ground contacts. We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation. A confirmatory study further indicates the necessity of synchronized avatar animations for virtual vehicles that rely on pedalling.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{UnbehaunSocial,
title = {Social Technology Appropriation in Dementia: Investigating the Role of Caregivers in engaging People with Dementia with a Videogame-based Training System},
author = {David Unbehaun (Uni Siegen) and Konstantin Aal (Uni Siegen) and Daryoush Daniel Vaziri (Hochschue Bonn-Rhein-Siegen) and Peter David Tolmie (Uni Siegen) and Rainer Wieching (Uni Siegen) and David Randall (Uni Siegen) and Volker Wulf (Uni Siegen)},
doi = {10.1145/3313831.3376648},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {We present here the outcomes of a 4-month evaluation of the individual, social and institutional impact of a videogame-based training system. The everyday behavior and interactions of 52 PwD and 25 caregivers was studied qualitatively, focusing on the role played by caregivers in integrating the system into daily routines.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{LaschkeMeaningful,
title = {Meaningful Technology at Work – A Reflective Design Case of Improving Radiologists’ Wellbeing Through Medical Technology},
author = {Matthias Laschke (Uni Siegen) and Christoph Braun (Siemens Healthineers) and Robin Neuhaus (Uni Siegen) and Marc Hassenzahl (Uni Siegen)},
doi = {10.1145/3313831.3376710},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The present paper presents a real-world case with a large medial technology provider, showing that medical technology could be designed more holistically to improve radiologists' wellbeing explicitly. Despite all skepticism, our prototypical applications resonated well among the radiologists involved, the healthcare provider, and other customers of the MTP.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{UhdeFairness,
title = {Fairness and Decision-making in Collaborative Shift Scheduling Systems},
author = {Alarith Uhde (Uni Siegen) and Nadine Schlicker (Ergosign GmbH) and Dieter P. Wallach (Ergosign GmbH) and Marc Hassenzahl (Uni Siegen)},
doi = {10.1145/3313831.3376656},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The strains associated with shift work decrease healthcare workers' well-being. However, shift schedules adapted to their individual needs can partially mitigate these problems. From a computing perspective, shift scheduling was so far mainly treated as an optimization problem with little attention given to the preferences, thoughts, and feelings of the healthcare workers involved. In the present study, we explore fairness as a central, human-oriented attribute of shift schedules as well as the scheduling process. Three in-depth qualitative interviews and a validating vignette study revealed that while on an abstract level healthcare workers agree on equality as the guiding norm for a fair schedule, specific scheduling conflicts should foremost be resolved by negotiating the importance of individual needs. We discuss elements of organizational fairness, including transparency and team spirit. Finally, we present a sketch for fair scheduling systems, summarizing key findings for designers in a readily usable way.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MelfiUnderstanding,
title = {Understanding what you feel: A Mobile Audio-Tactile System for Graphics Used at Schools with Students with Visual Impairment},
author = {Giuseppe Melfi (KIT-SZS) and Karin Müller (KIT-SZS) and Thorsten Schwarz (KIT-SZS) and Gerhard Jaworek (KIT-SZS) and Rainer Stiefelhagen (KIT-SZS)},
doi = {10.1145/3313831.3376508},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {A lot of information is nowadays presented graphically. However, students with blindness do not have access to visual information. Providing an alternative text is not always the appropriate solution as exploring graphics to discover information independently is a fundamental part of the learning process. In this work, we introduce a mobile audio-tactile learning environment, which facilitates the incorporation of real educational material. We evaluate our system by comparing three methods of interaction with tactile graphics: A tactile graphic augmented by (1) a document with key index information in Braille, (2) a digital document with key index information and (3) the TPad system, an audio-tactile solution meeting the specific needs within the school context. Our study shows that the TPad system is suitable for educational environments. Moreover, compared to the other methods TPad is faster to explore tactile graphics and it suggests a promising effect on the memorization of information.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{LangeHiveFive,
title = {HiveFive: Immersion Preserving Attention Guidance in Virtual Reality},
author = {Daniel Lange (University of Oldenburg) and Tim Claudius Stratmann (OFFIS - Institute for IT) and Uwe Gruenefeld (OFFIS - Institute for IT) and Susanne Boll (University of Oldenburg)},
url = {https://youtu.be/df_onXBj7cM, Video},
doi = {10.1145/3313831.3376803},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Recent advances in Virtual Reality (VR) technology, such as larger fields of view, have made VR increasingly immersive. However, a larger field of view often results in a user focusing on certain directions and missing relevant content presented elsewhere on the screen. With HiveFive, we propose a technique that uses swarm motion to guide user attention in VR. The goal is to seamlessly integrate directional cues into the scene without losing immersiveness. We evaluate HiveFive in two studies. First, we compare biological motion (from a prerecorded swarm) with non-biological motion (from an algorithm), finding further evidence that humans can distinguish between these motion types and that, contrary to our hypothesis, non-biological swarm motion results in significantly faster response times. Second, we compare HiveFive to four other techniques and show that it not only results in fast response times but also has the smallest negative effect on immersion.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KoelleSocial,
title = {Social Acceptability in HCI: A Survey of Methods, Measures, and Design Strategies},
author = {Marion Koelle (University of Oldenburg / Saarland University, Saarland Informatics Campus) and Swamy Ananthanarayan (University of Oldenburg) and Susanne Boll (University of Oldenburg)},
url = {https://www.twitter.com/hcioldenburg, Twitter},
doi = {10.1145/3313831.3376162},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {With the increasing ubiquity of personal devices, social acceptability of human-machine interactions has gained relevance and growing interest from the HCI community. Yet, there are no best practices or established methods for evaluating social acceptability. Design strategies for increasing social acceptability have been described and employed, but so far not been holistically appraised and evaluated. We offer a systematic literature analysis (N=69) of social acceptability in HCI and contribute a better understanding of current research practices, namely, methods employed, measures and design strategies. Our review identified an unbalanced distribution of study approaches, shortcomings in employed measures, and a lack of interweaving between empirical and artifact-creating approaches. The latter causes a discrepancy between design recommendations based on user research, and design strategies employed in artifact creation. Our survey lays the groundwork for a more nuanced evaluation of social acceptability, the development of best practices, and a future research agenda.},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KaulVibrotactile,
title = {Vibrotactile Funneling Illusion and Localization Performance on the Head},
author = {Oliver Beren Kaul (Leibniz University Hannover) and Michael Rohs (Leibniz University Hannover) and Benjamin Simon (Leibniz University Hannover) and Kerem Can Demir (Leibniz University Hannover) and Kamillo Ferry (Leibniz University Hannover)},
url = {https://youtu.be/emySptGIP9Y, Video},
doi = {10.1145/3313831.3376335},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The vibrotactile funneling illusion is the sensation of a single (non-existing) stimulus somewhere in-between the actual stimulus locations. Its occurrence depends upon body location, distance between the actuators, signal synchronization, and intensity. Related work has shown that the funneling illusion may occur on the forehead. We were able to reproduce these findings and explored five further regions to get a more complete picture of the occurrence of the funneling illusion on the head. The results of our study (24 participants) show that the actuator distance, for which the funneling illusion occurs, strongly depends upon the head region. Moreover, we evaluated the centralizing bias (smaller perceived than actual actuator distances) for different head regions, which also showed widely varying characteristics. We computed a detailed heat map of vibrotactile localization accuracies on the head. The results inform the design of future tactile head-mounted displays that aim to support the funneling illusion.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KatsiniTheRole,
title = {The Role of Eye Gaze in Security and Privacy Applications: Survey and Future HCI Research Directions},
author = {Christina Katsini (Human Opsis) and Yasmeen Abdrabou (Bundeswehr University Munich) and George E. Raptis (Human Opsis) and Mohamed Khamis (University of Glasgow) and Florian Alt (Bundeswehr University Munich)},
doi = {10.1145/3313831.3376840},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {For the past 20 years, researchers have investigated the use of eye tracking in security applications. We present a holistic view on gaze-based security applications. In particular, we canvassed the literature and classify the utility of gaze in security applications into a) authentication, b) privacy protection, and c) gaze monitoring during security critical tasks. This allows us to chart several research directions, most importantly 1) conducting field studies of implicit and explicit gaze-based authentication due to recent advances in eye tracking, 2) research on gaze-based privacy protection and gaze monitoring in security critical tasks which are under-investigated yet very promising areas, and 3) understanding the privacy implications of pervasive eye tracking. We discuss the most promising opportunities and most pressing challenges of eye tracking for security that will shape research in gaze-based security applications for the next decade.},
keywords = {Full Paper, Honorable Mention},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{MayerImproving,
title = {Improving Humans' Ability to Interpret Deictic Gestures in Virtual Reality},
author = {Sven Mayer (Carnegie Mellon University / University of Stuttgart) and Jens Reinhardt (Hamburg University of Applied Sciences) and Robin Schweigert (University of Stuttgart) and Brighten Jelke (Macalester College) and Valentin Schwind (University of Stuttgart / University of Regensburg) and Katrin Wolf (Hamburg University of Applied Sciences) and Niels Henze (University of Regensburg)},
url = {https://www.youtube.com/watch?v=Afi4TPzHdlM, Youtube},
doi = {10.1145/3313831.3376340},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Collaborative Virtual Environments (CVEs) offer unique opportunities for human communication. Humans can interact with each other over a distance in any environment and visual embodiment they want. Although deictic gestures are especially important as they can guide other humans' attention, humans make systematic errors when using and interpreting them. Recent work suggests that the interpretation of vertical deictic gestures can be significantly improved by warping the pointing arm. In this paper, we extend previous work by showing that models enable to also improve the interpretation of deictic gestures at targets all around the user. Through a study with 28 participants in a CVE, we analyzed the errors users make when interpreting deictic gestures. We derived a model that rotates the arm of a pointing user's avatar to improve the observing users' accuracy. A second study with 24 participants shows that we can improve observers' accuracy by 22.9%. As our approach is not noticeable for users, it improves their accuracy without requiring them to learn a new interaction technique or distracting from the experience.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KumarTagswipe,
title = {TAGSwipe: Touch Assisted Gaze Swipe for Text Entry},
author = {Chandan Kumar (University of Koblenz) and Ramin Hedeshy (University of Koblenz) and Scott MacKenzie (York University) and Steffen Staab (University of Stuttgart)},
url = {https://www.twitter.com/AnalyticComp},
doi = {10.1145/3313831.3376317},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The conventional dwell-based methods for text entry by gaze are typically slow and uncomfortable. A swipe-based method that maps gaze path into words offers an alternative. However, it requires the user to explicitly indicate the beginning and ending of a word, which is typically achieved by tedious gaze-only selection. This paper introduces TAGSwipe, a bi-modal method that combines the simplicity of touch with the speed of gaze for swiping through a word. The result is an efficient and comfortable dwell-free text entry method. In the lab study TAGSwipe achieved an average text entry rate of 15.46 wpm and significantly outperformed conventional swipe-based and dwell-based methods in efficacy and user satisfaction.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{SenguptaLeveraging,
title = {Leveraging Error Correction in Voice-based Text Entry by Talk-and-Gaze},
author = {Korok Sengupta (University of Koblenz) and Sabin Bhattarai (University of Koblenz) and Sayan Sarcar (University of Tsukuba) and Scott MacKenzie (York University) and Steffen Staab (University of Stuttgart)},
url = {https://www.twitter.com/AnalyticComp, Twitter},
doi = {10.1145/3313831.3376579},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {We present the design and evaluation of Talk-and-Gaze (TaG), a method for selecting and correcting errors with voice and gaze. TaG uses eye gaze to overcome the inability of voice- only systems to provide spatial information. The user’s point of gaze is used to select an erroneous word either by dwelling on the word for 800 ms (D-TaG) or by uttering a “select” voice command (V-TaG). A user study with 12 participants com- pared D-TaG, V-TaG, and a voice-only method for selecting and correcting words. Corrections were performed more than 20% faster with D-TaG compared to the V-TaG or voice-only methods. As well, D-TaG was observed to require 24% less selection effort than V-TaG and 11% less selection effort than voice-only error correction. D-TaG was well received in a subjective assessment with 66% of users choosing it as their preferred choice for error correction in voice-based text entry},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KrausAssessing,
title = {Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study},
author = {Matthias Kraus (University of Konstanz) and Katrin Angerbauer (University of Stuttgart) and Juri Buchmüller (University of Konstanz) and Daniel Schweitzer (University of Konstanz) and Daniel Keim (University of Konstanz) and Michael Sedlmair (University of Stuttgart) and Johannes Fuchs (University of Konstanz)},
url = {https://youtu.be/ybSj8ibu-qA, Video
https://www.twitter.com/dbvis, Twitter},
doi = {10.1145/3313831.3376675},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{SchubhanInvestigating,
title = {Investigating User-Created Gamification in an Image Tagging Task},
author = {Marc Schubhan (German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus) and Maximilian Altmeyer (German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus) and Dominic Buchheit (Saarland University) and Pascal Lessel (German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus)},
url = {https://www.youtube.com/watch?v=C_2RE_Tfzys, Video},
doi = {10.1145/3313831.3376360},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Commonly, gamification is designed by developers and not by end-users. In this paper we investigate an approach where users take control of this process. Firstly, users were asked to describe their own gamification concepts which would motivate them to put more effort into an image tagging task. We selected this task as gamification has already been shown to be effective here in previous work. Based on these descriptions, an implementation was made for each concept and given to the creator. In a between-subjects study (n=71), our approach was compared to a no-gamification condition and two conditions with fixed gamification settings. We found that providing participants with an implementation of their own concept significantly increased the amount of generated tags compared to the other conditions. Although the quality of tags was lower, the number of usable tags remained significantly higher in comparison, suggesting the usefulness of this approach.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{SchneegassBrainCode,
title = {BrainCoDe: Electroencephalography-based Comprehension Detection during Reading and Listening},
author = {Christina Schneegass (LMU Munich) and Thomas Kosch (LMU Munich) and Andrea Baumann (LMU Munich) and Marius Rusu (LMU Munich) and Mariam Hassib (Bundeswehr University Munich) and Heinrich Hussmann (LMU Munich)},
url = {https://www.twitter.com/mimuc, Twitter},
doi = {10.1145/3313831.3376707},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {The pervasive availability of media in foreign languages is a rich resource for language learning. However, learners are forced to interrupt media consumption whenever comprehension problems occur. We present BrainCoDe, a method to implicitly detect vocabulary gaps through the evaluation of event-related potentials (ERPs). In a user study (N=16), we evaluate BrainCoDe by investigating differences in ERP amplitudes during listening and reading of known words compared to unknown words. We found significant deviations in N400 amplitudes during reading and in N100 amplitudes during listening when encountering unknown words. To evaluate the feasibility of ERPs for real-time applications, we trained a classifier that detects vocabulary gaps with an accuracy of 87.13% for reading and 82.64% for listening, identifying eight out of ten words correctly as known or unknown. We show the potential of BrainCoDe to support media learning through instant translations or by generating personalized learning content.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{HoppeAHumanTouch,
title = {A Human Touch: Social Touch Increases the Perceived Human-likeness of Agents in Virtual Reality},
author = {Matthias Hoppe (LMU Munich) and Beat Rossmy (LMU Munich) and Daniel Peter Neumann (LMU Munich) and Stephan Streuber (University of Konstanz) and Albrecht Schmidt (LMU Munich) and Tonja Machulla (LMU Munich)},
url = {https://www.twitter.com/mimuc, Twitter},
doi = {10.1145/3313831.3376719},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Virtual Reality experiences and games present believable virtual environments based on graphical quality, spatial audio, and interactivity. The interaction with in-game characters, controlled by computers (agents) or humans (avatars), is an important part of VR experiences. Pre-captured motion sequences increase the visual humanoid resemblance. However, this still precludes realistic social interactions (eye contact, imitation of body language), particularly for agents. We aim to make social interaction more realistic via social touch. Social touch is non-verbal, conveys feelings and signals (coexistence, closure, intimacy). In our research, we created an artificial hand to apply social touch in a repeatable and controlled fashion to investigate its effect on the perceived human-likeness of avatars and agents. Our results show that social touch is effective to further blur the boundary between computer- and human-controlled virtual characters and contributes to experiences that closely resemble human-to-human interactions.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{KoschRSVP,
title = {One does not Simply RSVP: Mental Workload to Select Speed Reading Parameters using Electroencephalography},
author = {Thomas Kosch (LMU Munich) and Albrecht Schmidt (LMU Munich) and Simon Thanheiser (LMU Munich) and Lewis L. Chuang (LMU Munich)},
url = {https://www.twitter.com/mimuc, Twitter},
doi = {10.1145/3313831.3376766},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Rapid Serial Visual Presentation (RSVP) has gained popularity as a method for presenting text on wearable devices with limited screen space. Nonetheless, it remains unclear how to calibrate RSVP display parameters, such as spatial alignments or presentation rates, to suit the reader’s information processing ability at high presentation speeds. Existing methods rely on comprehension and subjective workload scores, which are influenced by the user’s knowledge base and subjective perception. Here, we use electroencephalography (EEG) to directly determine how individual information processing varies with changes in RSVP display parameters. Eighteen participants read text excerpts with RSVP in a repeated-measures design that manipulated the Text Alignment and Presentation Speed of text representation. We evaluated how predictive EEG metrics were of gains in reading speed, subjective workload, and text comprehension. We found significant correlations between EEG and increasing Presentation Speeds and propose how EEG can be used for dynamic selection of RSVP parameters.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{VilleVirtual,
title = {Virtual Field Studies: Conducting Studies on Public Displays in Virtual Reality},
author = {Ville Mäkelä (LMU Munich / Tampere University) and Rivu Radiah (Bundeswehr University Munich) and Saleh Alsherif (German University in Cairo) and Mohamed Khamis (University of Glasgow) and Chong Xiao (LMU Munich) and Lisa Borchert (LMU Munich) and Albrecht Schmidt (LMU Munich) and Florian Alt (Bundeswehr University Munich)},
url = {https://www.twitter.com/mimuc, Twitter},
doi = {10.1145/3313831.3376796},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Field studies on public displays can be difficult, expensive, and time-consuming. We investigate the feasibility of using virtual reality (VR) as a test-bed to evaluate deployments of public displays. Specifically, we investigate whether results from virtual field studies, conducted in a virtual public space, would match the results from a corresponding real-world setting. We report on two empirical user studies where we compared audience behavior around a virtual public display in the virtual world to audience behavior around a real public display. We found that virtual field studies can be a powerful research tool, as in both studies we observed largely similar behavior between the settings. We discuss the opportunities, challenges, and limitations of using virtual reality to conduct field studies, and provide lessons learned from our work that can help researchers decide whether to employ VR in their research and what factors to account for if doing so.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}
@inproceedings{VoelkelHow,
title = {How to Trick AI: Users’ Strategies for Protecting Themselves From Automatic Personality Assessment},
author = {Sarah Theres Völkel (LMU Munich) and Renate Häuslschmid (Madeira Interactive Technologies Institute) and Anna Werner (LMU Munich) and Heinrich Hussmann (LMU Munich) and Andreas Butz (LMU Munich)},
url = {https://www.twitter.com/mimuc, Twitter},
doi = {10.1145/3313831.3376877},
year  = {2020},
date = {2020-04-26},
booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems. CHI 2020},
publisher = {ACM},
abstract = {Psychological targeting tries to influence and manipulate users' behaviour. We investigated whether users can protect themselves from being profiled by a chatbot, which automatically assesses users' personality. Participants interacted twice with the chatbot: (1) They chatted for 45 minutes in customer service scenarios and received their actual profile (baseline). (2) They then were asked to repeat the interaction and to disguise their personality by strategically tricking the chatbot into calculating a falsified profile. In interviews, participants mentioned 41 different strategies but could only apply a subset of them in the interaction. They were able to manipulate all Big Five personality dimensions by nearly 10%. Participants regarded personality as very sensitive data. As they found tricking the AI too exhaustive for everyday use, we reflect on opportunities for privacy protective designs in the context of personality-aware systems.},
keywords = {Full Paper},
pubstate = {published},
tppubtype = {inproceedings}
}